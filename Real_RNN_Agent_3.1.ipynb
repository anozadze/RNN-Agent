{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Iterative Prisoner's Dilemma\n",
    "Das *Gefangenendilemma* oder *Prisoner's Dilemma* ist eines der bekanntesten Probleme der Spieltheorie (welche für Multi-Agenten Systeme im Bereich der KI relevant ist). Initial von Thomas Hobbes in der politischen Philosophie ([Der Leviathan](https://books.google.de/books/about/Der_Leviathan.html?id=FGAGEAAAQBAJ&redir_esc=y)) eingeführt, handelt das Dilemma von zwei Verbrechern, die von der Polizei gefangen genommen und unabhängig verhöhrt werden. Jeder Gefangene kann nun entweder mit dem anderen Gefangenen kooperieren, also der Polizei nichts verraten (*Cooperate*) oder den anderen Gefangenen im Stich lassen (*Defect*). Halten beide Gefangene dicht, so müssen sie für kurze Zeit (z.B. 2 Jahre) ins Gefängnis, verraten sich beide gegenseitig müssen beide länger (z.B. 5 Jahre) ins Gefängnis, hält aber einer dicht und der andere nicht, so kommt der eine für die längste Zeit (z.B. 8 Jahre), der andere aber gar nicht (also 0 Jahre) ins Gefängnis. Das Gefangenendilemma lässt sich also als eine Pay-Off Matrix der folgenden Form schreiben:\n",
    "\n",
    "| | Cooperate | Defect |\n",
    "| :- | :--: | :-: |\n",
    "| **Cooperate** | -2 / -2 | -8 / 0 |\n",
    "| **Defect** | 0 / -8 | -5 / -5 |\n",
    "\n",
    "Dabei sind horizontal die Aktionen von Agent 1 und vertikal die Aktionen von Agent 2 aufgetragen. Links des / ist der Reward von Agent 1, rechts des / der Reward von Agent 2. Es wird davon ausgegangen, dass die Agenten ihre Aktionen gleichzeitig wählen und sofort einen Reward bekommen.\n",
    "\n",
    "Das **Dilemma** ergibt sich dabei folgendermaßen: Für einen einzelnen Agenten ist die Kombination ich wähle *Defect*, der andere wählt *Cooperate* am besten, für beide Agenten zusammengerechnet ist aber die Situation beide wählen *Cooperate* am besten. Zugleich ist die beste Antwort auf jede Aktion des Gegners eigentlich diesen zu verraten, *Defect*/*Defect* ist daher ein [Nash Equillibrium](https://de.wikipedia.org/wiki/Nash-Gleichgewicht), aber suboptimal für beide Agenten zusammen, *Cooperate/Cooperate* ist [Pareto optimal](https://de.wikipedia.org/wiki/Pareto-Optimum). Daher stellt sich im Prisoner's Dilemma die Frage wie sehr ich darauf vertraue, dass der andere zu mir hält und, ob ich das ausnutzen möchte.\n",
    "\n",
    "Gerade dieses Spannungsfeld wird beim *iterativen Gefangenendilemma* umso wichtiger. Während das klassiche Gefangenendilemma nur für eine \"Runde\" gespielt wird, wiederholt man bei der iterativen Version die Aktionsauswahl mehrere Male. Nutzt man die Gutmütigkeit des Gegenspielers in dieser Version einmal aus, so wird dieser zukünftig nicht mehr so gutmütig sein. Das *iterative Gefangenendilemma* modelliert also den Aufbau und das Ausnutzen von Vertrauen in einem kooperativenm Multi-Agenten System. Dieses Modell wurde zum Beispiel in der Politikwissenschaft verwendet, um Verhaltensweisen im Kalten Krieg zu diskutieren. Es gibt aber noch viele weitere Arbeiten zum iterativen Gefangenendilemma aus der Spieltheorie, Informatik, Psychologie, Politikwissenschaft, Soziologie und Philosophie. Eine spannende und einfache Einführung in dieses Thema findet sich in [diesem Video](https://www.youtube.com/watch?v=mScpHTIi-kM).\n",
    "\n",
    "**In dieser Challenge sollt ihr einen Agenten entwickeln, der, einer bestimmten Strategie folgend, im iterativen Gefangenendilemma möglichst viel kumulierten Reward erzielt.** Grob zusammengefasst gibt es vier Klassen solcher Agenten:\n",
    "- Reflex-Agenten (if-then Regeln)\n",
    "- Modell-basierte Reflex-Agenten (if-then Regeln basierend auf der Historie)\n",
    "- lernende Gegenspieler-Modell-basierte Agenten (lernen Aktionen des Gegenspielers vorherzusagen)\n",
    "- lernende Wert-basierte Agenten (lernen den Wert einer Aktion in einer bestimmten Situation vorherzusagen und wählen wertigste Aktion)\n",
    "\n",
    "Um das Spiel ein wenig spannender zu gestalten wurden Unsicherheiten in diese Implementierung eingebaut, die es im klassischen iterativen Prisoner's Dilemma nicht gibt:\n",
    "- Für jedes Spiel werden neue Rewards für jedes Aktions-Paar gezogen (aus einer Normalverteilung um die oben angeführten Werte mit Varianz 1) und mit einer Wahrscheinlichkeit von 1% ändern sich diese Werte bei jeder Reward Berechnung, werden also neu gesamplet\n",
    "- Die Rewards sind nicht fix, sondern werden aus einer Normalverteilung mit Varianz 1, die über den fixen Rewards zentriert ist gesamplet\n",
    "- Mit einer Wahrscheinlichkeit von 10% observiert euer Agent die gegenteilige Aktion, die der Gegner tatsächlich gemacht hat\n",
    "\n",
    "Die Payoff Matrix für die vorliegende Challenge kann also wie folgt geschrieben werden (wobei N(mu, var) für ein Sample aus einer Normalverteilung mit Mittelwert mu und Varianz var steht, beachtet aber, dass aus der inneren Normalverteilung nur mit einer Wahrscheinlichkeit von 1% ein neur Wert gezogen wird und dieser sonst konstant bleibt, während aus der äußeren Normalverteilung in jeder Runde neu gezogen wird):\n",
    "\n",
    "| | Cooperate | Defect |\n",
    "| :- | :--: | :-: |\n",
    "| **Cooperate** | N(N(-2, 1), 1) / N(N(-2, 1), 1) | N(N(-8, 1), 1) / N(N(0, 1), 1) |\n",
    "| **Defect** | N(N(0, 1), 1) / N(N(-8, 1), 1) | N(N(-5, 1), 1) / N(N(-5, 1), 1) |\n",
    "\n",
    "Als Eingabe für die Entscheidung erhalten eure Agenten nur die Aktion des Gegners, diese und eure eigenen Aktionen dürfen sich aber gemerkt werden und daraus können wahrscheinliche Rewards abgeleitet werden. Bitte versucht aber nicht auf andere Variablen außerhalb eures Agenten zuzugreifen!\n",
    "\n",
    "\n",
    "## Da Rulez\n",
    "Den Code für den Wettkampf (Tournament) findet ihr unten, dieser wird auch nicht abgeändert und in dieser Form für die finale Auswertung verwendet. Im Allgemeinen gilt:\n",
    "\n",
    "- im Tournament spielt am Ende jeder Agent 100x gegen jeden Agenten inklusive sich selbst und vier Baselines\n",
    "- ihr dürfte euch zu zweit (aber nicht zu mehreren) zusammen tun, müsst dann den Gewinn aber auch teilen\n",
    "- wer im Schnitt am meisten Reward bekommt, also die wenigsten Jahre im Gefängnis verbringen muss, gewinnt\n",
    "- jede Runde geht zwischen 1000 und 2000 Zeitschritte, in jedem Zeitschritt wählt jeder der Spielenden Agenten genau eine Aktion\n",
    "- Agenten dürfen nur eine der zwei vorgegebenen Aktionen und nichts anderes zurückgeben\n",
    "- ihr dürft eure Agenten beliebig implementieren, aber die Signaturen des Konstruktors und der `get_next_action` Methode dürfen nicht geändert werden, das heißt weder die Eingabe noch die Ausagebparameter dürfen angepasst werden\n",
    "- ihr dürft keine \"hacky\" Tricks verwenden, um auf externe Werte zuzugreifen, auf die euer Agent nicht zugreifen können sollte oder den Agenten des Gegners durch Programmierkniffe zu behindern\n",
    "- ihr SOLLT spieltheoretische, statistische, mathematische und sonstige Eigenschaften der vorgegebenen Implementierung ausnutzen, um einen möglichst guten Agenten zu finden \n",
    "- Bitte versucht eure Implementierung möglichst effizient zu halten und seht z.B. von riesigen Neuronalen Netzen ab, ich möchte das Tournament schnellstmöglich auswerten könnwn\n",
    "- der Zeitrahmen für die Implementierung ist von Freitag, 10:00 Uhr bis Freitag, 13:00 UHr, die Abgabe [in diesem Google Drive Ordner](https://drive.google.com/drive/folders/1JwScCpgj43v4yqHE6la3KO7XdwKKUGwE?usp=sharing) am Freitag um 13:00 Uhr wird gewertet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenten\n",
    "Jeder Agent muss in jedem Zeitschritt zwischen zwei Aktionen wählen, `Cooperate` und `Defect` (siehe oben). Um diese Auswahlmöglichkeit programmatisch umzusetzen, wählen wir ein [Enum](https://en.wikipedia.org/wiki/Enumerated_type). Das heißt, zu jedem Zeitpunkt muss ein Agent `Action.COOPERATE` oder `Action.DEFECT` zurückgeben. Andere Rückgaben sind nicht zulässig und führen zur Disqualifikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Action(Enum):\n",
    "    COOPERATE = 1\n",
    "    DEFECT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes folgt die Basis-Klasse für alle Agenten. Alle Agenten im Tournament sollten von dieser Klasse erben. Die Basisklasse stellt sicher, dass die übergebene letzte Aktion des Gegenspielers eine valide Aktion oder None (nur im ersten Durchlauf erlaubt) ist. Bitte beachtet die Methoden Signaturen, jeder Agent braucht einen Konstruktor (`__init__`) ohne Eingabeparameter und eine `get_next_action` Methode, die eine `Action` oder None übergeben bekommt und eine `Action` zurückgibt (`None` als Rückgabe ist, anders als in der Basisklasse, für eure Agenten nicht zulässig!!). Andere Übergabeparameter und Rückgabewerte sind für diese Methoden nicht erlaubt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        # last_opponent_action must be one of Action.COOPERATE, Action.DEFECT, None\n",
    "        assert last_opponent_action in list(Action) or last_opponent_action is None\n",
    "        return None  # None return is not allowed and will lead to disqualification, return either Action.COOPERATE or ACTION.DEFECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgenden sind vier Baselines implementiert, die euch einen Hinweis darauf geben können, wie ihr eure Agenten implementieren könnt. Diese Baselines werden auch am Tournament teilnehmen, ihre Schwächen auszunutzen kann also Punkte bringen! Die folgenden Baselines sind implementiert:\n",
    "\n",
    "- `DefectAgent`: wählt, egal was der Gegner tut, immer `Action.DEFECT`, spielt also das Nash Equilibrium des nicht-iterativen Gefangenendilemmas\n",
    "- `CooperateAgent`: wählt, egal was der Gegner tut, immer `Action.COOPERATE`, hofft also auf einen anderen Agenten, der ebenfalls zusammenarbeiten will\n",
    "- `RandomAgent`: wählt immer eine zufällige Aktion, ist daher unberechenbar\n",
    "- `SwitchingAgent`: wählt `Action.DEFECT`und `Action.COOPERATE` immer im Wechsel, wobei die erste Aktion zufällig gewählt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\"\"\" Always Defect baseline \"\"\"\n",
    "class DefectAgent(Agent):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "        return Action.DEFECT\n",
    "\n",
    "\"\"\" Always Cooperate baseline \"\"\"\n",
    "class CooperateAgent(Agent):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "        return Action.COOPERATE\n",
    "\n",
    "\"\"\" Random Action Selection baseline \"\"\"\n",
    "class RandomAgent(Agent):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "        return random.choice(list(Action))\n",
    "\n",
    "\"\"\" Constant switching baseline \"\"\"\n",
    "class SwitchingAgent(Agent):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.my_last_action = random.choice(list(Action))\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "        \n",
    "        if self.my_last_action == Action.COOPERATE:\n",
    "            self.my_last_action = Action.DEFECT\n",
    "        else:\n",
    "            self.my_last_action = Action.COOPERATE\n",
    "        \n",
    "        return self.my_last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLineAgent(Agent):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "        if last_opponent_action is None:\n",
    "            return Action.COOPERATE\n",
    "        return last_opponent_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAgent(Agent):\n",
    "    round = 0\n",
    "    opponent_defects = 0\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "        self.round += 1\n",
    "        if last_opponent_action is Action.DEFECT: self.opponent_defects += 1\n",
    "        if self.opponent_defects < (self.round / 10):\n",
    "            return Action.COOPERATE\n",
    "        return Action.DEFECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class der_Morser_Agent(Agent):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.entscheidungen = []\n",
    "        self.meine_entscheidungen = [Action.DEFECT,Action.DEFECT,Action.COOPERATE,Action.COOPERATE,Action.DEFECT,Action.DEFECT,Action.COOPERATE,Action.COOPERATE,Action.DEFECT,Action.DEFECT]\n",
    "        # print(self.meine_entscheidungen)\n",
    "        self.Gegenstrategie = None\n",
    "        pass\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "        \n",
    "        if  last_opponent_action != None:\n",
    "            self.entscheidungen.append(last_opponent_action)\n",
    "        \n",
    "        i = len(self.entscheidungen)\n",
    "        \n",
    "        if i < 10:\n",
    "            return self.meine_entscheidungen[i]\n",
    "        \n",
    "        elif i == 10:\n",
    "            \"\"\"\n",
    "            counter_wir = 0\n",
    "            for i in range(10):\n",
    "                if self.meine_entscheidungen[i] == self.entscheidungen[i]:\n",
    "                    counter_wir += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "            counter_D = 0\n",
    "            for i in range(10):\n",
    "                if self.entscheidungen[i] == Action.DEFECT:\n",
    "                    counter_D += 1\n",
    "                    \n",
    "            \n",
    "            counter_C = 0\n",
    "            for i in range(10):\n",
    "                if self.entscheidungen[i] == Action.COOPERATE:\n",
    "                    counter_C += 1\n",
    "                    \n",
    "                \n",
    "            counter_A = 0\n",
    "            for i in range(10):\n",
    "                if i > 0 and self.entscheidungen[i] != self.entscheidungen[i-1]:\n",
    "                    counter_A += 1\n",
    "            \n",
    "                \n",
    "            liste = [counter_wir,counter_D,counter_C,counter_A]\n",
    "            \n",
    "            max_index = liste.index(max(liste))\n",
    "            if liste(max_index) >= 8:\n",
    "                self.Gegenstrategie = max_index\n",
    "            \"\"\"\n",
    "            self.Gegenstrategie = self.test_strategie()\n",
    "            if self.Gegenstrategie == 0:\n",
    "                return Action.COOPERATE\n",
    "            elif self.Gegenstrategie == None:\n",
    "                return Action.DEFECT\n",
    "            else:\n",
    "                return Action.DEFECT\n",
    "    \n",
    "        else:\n",
    "            if self.Gegenstrategie == 0:\n",
    "                return Action.COOPERATE\n",
    "            elif self.Gegenstrategie == None:\n",
    "                return Action.DEFECT\n",
    "            else:\n",
    "                return Action.DEFECT\n",
    "            \n",
    "    def test_strategie(self):\n",
    "        counter_wir = 0\n",
    "        for i in range(10):\n",
    "            if self.meine_entscheidungen[i] == self.entscheidungen[i]:\n",
    "                counter_wir += 1\n",
    "\n",
    "\n",
    "\n",
    "        counter_D = 0\n",
    "        for i in range(10):\n",
    "            if self.entscheidungen[i] == Action.DEFECT:\n",
    "                counter_D += 1\n",
    "\n",
    "\n",
    "        counter_C = 0\n",
    "        for i in range(10):\n",
    "            if self.entscheidungen[i] == Action.COOPERATE:\n",
    "                counter_C += 1\n",
    "\n",
    "\n",
    "        counter_A = 0\n",
    "        for i in range(10):\n",
    "            if i > 0 and self.entscheidungen[i] != self.entscheidungen[i-1]:\n",
    "                counter_A += 1\n",
    "\n",
    "\n",
    "        liste = [counter_wir,counter_D,counter_C,counter_A]\n",
    "\n",
    "        max_index = liste.index(max(liste))\n",
    "        if liste[max_index] >= 8:\n",
    "            self.Gegenstrategie = max_index\n",
    "        \n",
    "        return self.Gegenstrategie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YouBetterPlayNice(Agent):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.opponents_last_action = Action.COOPERATE\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "        \n",
    "        my_action = Action.COOPERATE\n",
    "        \n",
    "        if last_opponent_action == Action.DEFECT and self.opponents_last_action == Action.DEFECT :\n",
    "            my_action = Action.DEFECT\n",
    "        \n",
    "        self.opponents_last_action = last_opponent_action\n",
    "        return my_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Agent_007(Agent):\n",
    "    def __init__(self) -> None:\n",
    "        self.hidden_state = float(51)\n",
    "        self.lower_limit = float(0)\n",
    "        self.lower_bound = float(50)\n",
    "        self.upper_bound = float(60)\n",
    "        self.upper_limit = float(100)\n",
    "\n",
    "    def adjust_to_limits(self, h):\n",
    "        if h >= self.upper_limit:\n",
    "            h = self.upper_limit\n",
    "        elif h <= self.lower_limit:\n",
    "            h = self.lower_limit\n",
    "        return h\n",
    "\n",
    "    def update_hidden_state(self, h) -> float:\n",
    "        self.hidden_state = h\n",
    "        return self.hidden_state\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "\n",
    "        h = self.hidden_state\n",
    "        if last_opponent_action == Action.COOPERATE:\n",
    "            update = random.gauss(mu=1, sigma=1)\n",
    "            # update = 1\n",
    "        else:\n",
    "            update = random.gauss(mu=-1, sigma=1)\n",
    "            # update = -1\n",
    "        h += update\n",
    "\n",
    "        h = self.adjust_to_limits(h)\n",
    "        self.hidden_state = self.update_hidden_state(h)\n",
    "        # print(\"the current hidden state is: \", self.hidden_state)\n",
    "\n",
    "        if self.hidden_state <= self.lower_bound or self.hidden_state >= self.upper_bound:\n",
    "            return Action.DEFECT\n",
    "        else:\n",
    "            return Action.COOPERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Emmas innocent Agent \"\"\"\n",
    "class dilEmma(Agent):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.opponent_actions = []\n",
    "        self.round_count = 0\n",
    "        self.test_instance = 0\n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        self.round_count += 1\n",
    "        if self.round_count % 100 == 0:\n",
    "            self.test_instance = self.round_count\n",
    "\n",
    "        # Defect the first 10 rounds\n",
    "        if self.test_instance <= self.round_count <= self.test_instance + 10:\n",
    "            return Action.DEFECT\n",
    "        \n",
    "        # Cooperate 10 times\n",
    "        if self.test_instance + 10 < self.round_count <= self.test_instance + 20:\n",
    "            return Action.COOPERATE\n",
    "        \n",
    "        # Evaluate strategy after round 20\n",
    "        if self.round_count > self.test_instance + 20:\n",
    "            identical_test = sum(action == Action.COOPERATE for action in self.opponent_actions[self.test_instance:self.test_instance+20])/20\n",
    "            cooperation_test = sum(action == Action.COOPERATE for action in self.opponent_actions[self.test_instance+10:self.test_instance+20]) \n",
    "\n",
    "            if identical_test >= 0.8 or identical_test <= 0.8: \n",
    "                return Action.DEFECT\n",
    "            \n",
    "            if cooperation_test >= 7 and last_opponent_action == Action.COOPERATE:\n",
    "                rand_num = random.randint(0, 9)\n",
    "                if rand_num < 9:  # 90% chance to cooperate\n",
    "                    return Action.COOPERATE\n",
    "                else:\n",
    "                    return Action.DEFECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sollt ihr euren Agenten implementieren. Bestenfalls erbt euer Agent von der `Agent`. Die Übergabe- und Rückgabeparameter für die `__init__` und die `get_next_action` Methoden sollten gleich zu den obigen Implementierungen sein. Denkt euch gerne einen coolen (und eindeutigen) Namen für eure Klasse/euren Agenten aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the RNN model\n",
    "class Real_RNN_Agent_31(Agent):\n",
    "    def __init__(self) -> None:\n",
    "    # def __init__(self, input_size = 2, hidden_size = 16, output_size = 2) -> None:\n",
    "\n",
    "        self.rounds = 1\n",
    "        self.print_rounds = 1\n",
    "        self.print_checker = 1\n",
    "\n",
    "        self.generate_starting_sequence()\n",
    "\n",
    "        # self.starting_sequence = []\n",
    "        # for _ in range(0,10):\n",
    "        #     if random.random() > 0.5:\n",
    "        #         self.starting_sequence.append(Action.COOPERATE)\n",
    "        #     else:\n",
    "        #         self.starting_sequence.append(Action.DEFECT)\n",
    "        # print(\"my starting sequence is: \", self.starting_sequence)\n",
    "\n",
    "        # Initialize the RNN\n",
    "\n",
    "        # LookerUp-Depth (on how many iterations is the RNN trained at once?)\n",
    "        self.LU_Depth = 10\n",
    "        # Prediction-Depth (how many steps into the future should the RNN predict?)\n",
    "        self.Pred_Depth = 5\n",
    "        # batch-size (Default is 1)\n",
    "        self.batch_size = 1\n",
    "\n",
    "        self.input_size = 2*2*self.LU_Depth # 20 input layers\n",
    "        self.hidden_size = 2*self.input_size # 40 hidden layers\n",
    "        self.output_size = 2*self.Pred_Depth # 20 output layers\n",
    "        self.rnn = nn.Sequential(\n",
    "            nn.Linear(self.input_size + self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # epoch_length = 10000\n",
    "        # epoch = 0\n",
    "        \n",
    "        self.my_moves = []\n",
    "        self.their_moves = []\n",
    "\n",
    "        # initialize hidden state\n",
    "        self.hidden_state = self.init_hidden_state()\n",
    "        # self.hidden_size = torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "        self.i2h = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
    "        self.i2o = nn.Linear(self.input_size + self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # initialize backpropagation parameters\n",
    "        self.total_rewards = 0\n",
    "        # self.backprop_epoch = 0\n",
    "        # self.total_loss = 0\n",
    "        self.current_losses = []\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        # self.criterion = nn.MSELoss()\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        self.optimizer = optim.SGD(self.rnn.parameters(), lr=1e-0) # lr=1e-3)\n",
    "        # weight decay introduces L2 regularization\n",
    "        # self.optimizer = optim.Adam(self.rnn.parameters(), lr=1e-1, weight_decay=1e-3, amsgrad=True) # lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "        print(\"initialized\")\n",
    "    \n",
    "    # initialize hidden state\n",
    "    def init_hidden_state(self):\n",
    "        # return torch.zeros(self.batch_size, self.hidden_size)\n",
    "        # return torch.ones(self.batch_size, self.hidden_size)*0.5\n",
    "        return torch.rand(self.batch_size, self.hidden_size)\n",
    "\n",
    "    def generate_starting_sequence(self):\n",
    "\n",
    "        self.starting_sequence = []\n",
    "        random.seed(12102003)\n",
    "        \n",
    "        for _ in range(0,20):\n",
    "            if random.random() > 0.5:\n",
    "                self.starting_sequence.append(Action.COOPERATE)\n",
    "            else:\n",
    "                self.starting_sequence.append(Action.DEFECT)\n",
    "        # print(\"my starting sequence is: \", self.starting_sequence)\n",
    "\n",
    "        return self.starting_sequence\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    # Define the Prisoner's Dilemma game logic\n",
    "    def get_reward(self, player1, player2):\n",
    "\n",
    "        if player1 == Action.COOPERATE and player2 == Action.COOPERATE:\n",
    "            outcome = 2\n",
    "        elif player1 == Action.COOPERATE and player2 == Action.DEFECT:\n",
    "            outcome = 8\n",
    "        elif player1 == Action.DEFECT and player2 == Action.COOPERATE:\n",
    "            outcome = 0\n",
    "        elif player1 == Action.DEFECT and player2 == Action.DEFECT:\n",
    "            outcome = 5\n",
    "\n",
    "        # payoff_matrix = {\n",
    "        # (Action.COOPERATE, Action.COOPERATE): 2,\n",
    "        # (Action.COOPERATE, Action.DEFECT): 0,\n",
    "        # (Action.DEFECT, Action.COOPERATE): 8,\n",
    "        # (Action.DEFECT, Action.DEFECT): 5}\n",
    "        # outcome = payoff_matrix[player1, player2]\n",
    "            \n",
    "        return outcome\n",
    "    \n",
    "    def calculate_rewards(self, my_move, their_move):\n",
    "        \n",
    "        self.total_rewards = self.get_reward(my_move, their_move)\n",
    "\n",
    "        # to check the current reward via print\n",
    "        # if self.rounds % 2000 == 0:\n",
    "        #     the_current_rewards = []\n",
    "        #     for ii in range(len(my_move)):\n",
    "        #         current_reward = self.get_reward(my_move[ii], their_move[ii])\n",
    "        #         self.total_rewards += current_reward\n",
    "        #         the_current_rewards.append(current_reward)\n",
    "        #     print(\"\\n current rewards: \" + str(the_current_rewards.float()) + \"\\n\")\n",
    "\n",
    "        # else:\n",
    "        # for ii in range(len(my_move)):\n",
    "        #     self.total_rewards += self.get_reward(my_move[ii], their_move[ii])\n",
    "\n",
    "        return self.total_rewards\n",
    "    \n",
    "    # Function to convert moves to one-hot vectors\n",
    "    def input_to_onehot(self, move):\n",
    "\n",
    "        if move == Action.COOPERATE:\n",
    "            return torch.tensor([[1, 0]], dtype=torch.float32)\n",
    "        elif move == Action.DEFECT:\n",
    "            return torch.tensor([[0, 1]], dtype=torch.float32)\n",
    "\n",
    "    # def input_to_target(self, next_move):\n",
    "\n",
    "    #     if next_move == Action.COOPERATE:\n",
    "    #         target_move = torch.tensor([[1, 0]], dtype=torch.float32)\n",
    "    #     else:\n",
    "    #         target_move = torch.tensor([[0, 1]], dtype=torch.float32)\n",
    "    #     return target_move\n",
    "    \n",
    "    # Backpropagation\n",
    "    def backward(self, output, my_next_move, their_next_move):\n",
    "\n",
    "        # calculate classification loss:\n",
    "        classification_loss = self.criterion(output, self.input_to_onehot(their_next_move))\n",
    "\n",
    "        current_reward = self.get_reward(my_next_move, their_next_move)\n",
    "        reward_loss = self.criterion(torch.tensor(current_reward, dtype=torch.float32, requires_grad=True).squeeze(), torch.zeros(1))\n",
    "\n",
    "        # Weighted loss incorporating rewards\n",
    "        total_loss1 = classification_loss # only classification loss\n",
    "        total_loss2 = reward_loss  # only mean rewards as weights\n",
    "        # total_loss3 = classification_loss + reward_loss  # both mean rewards and class loss as weights\n",
    "        \n",
    "        self.total_loss = total_loss2\n",
    "        \n",
    "        # Perform backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        self.total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # total_loss += loss.item()\n",
    "        self.current_losses.append(self.total_loss.item())\n",
    "\n",
    "        # self.backprop_epoch += 1\n",
    "\n",
    "        pass\n",
    "\n",
    "    def print_stuff(self):\n",
    "\n",
    "        the_current_rewards = []\n",
    "        # sometimes we play as the opponent (first Action is None), sometimes not (first Action is filled).\n",
    "        # move_diff accounts for the difference in length.\n",
    "        move_diff = len(self.my_moves) - len(self.their_moves)\n",
    "        # print(move_diff)\n",
    "        # print(len(self.my_moves))\n",
    "        # print(len(self.their_moves))\n",
    "        # print(self.rounds)\n",
    "        \n",
    "        for ii in range(-len(self.starting_sequence),0,1):\n",
    "            the_current_rewards.append(self.get_reward(self.my_moves[ii-move_diff], self.their_moves[ii]))\n",
    "                \n",
    "        print(\"\\n current rewards: \" + str(the_current_rewards))\n",
    "\n",
    "        print(\" current losses: \" + str(self.current_losses) + \"\\n\")\n",
    "\n",
    "        # print(\"game iteration: \" + str(self.rounds)  + \"\\n\")\n",
    "\n",
    "        pass\n",
    "\n",
    "    def check_morse(self):\n",
    "\n",
    "        # \"Morser part\": check if I am playing against myself\n",
    "        morse = False\n",
    "        # counts overlaps between the first 10 elements of their_moves and our starting sequence\n",
    "        checker1 = sum([x == y for x, y in zip(self.their_moves[0:len(self.starting_sequence):1], self.starting_sequence[0:len(self.starting_sequence):1])])\n",
    "\n",
    "        if checker1 > 17:\n",
    "            morse = True\n",
    "        else:\n",
    "            morse = False\n",
    "\n",
    "        return morse\n",
    "\n",
    "    def check_fairplay(self):\n",
    "        \n",
    "        # sanity check in case the opponent is still someone else (or in case a new round has begun without notice)\n",
    "        morse = True\n",
    "        checker2 = sum([True if var == Action.DEFECT else False for var in self.their_moves[-int(len(self.starting_sequence)/2):]])\n",
    "        \n",
    "        if checker2 > 3:\n",
    "            morse = False\n",
    "            # something with self.print_checker?\n",
    "            # print(\"\\n greetings from checker2 \\n\")\n",
    "        else:\n",
    "            morse = True\n",
    "\n",
    "        return morse\n",
    "\n",
    "        # if checker1 > 7:\n",
    "        #     morse = True\n",
    "        #     \n",
    "        #     if self.rounds > 2*len(self.starting_sequence):\n",
    "        #         checker2 = sum([True if _ == Action.DEFECT else False for _ in self.their_moves[-10:]])\n",
    "        #         if checker2 > 3:\n",
    "        #             morse = False\n",
    "        #             self.rounds = 1\n",
    "        #             print(\"\\n greetings from checker \\n\")\n",
    "        #             self.my_moves = []\n",
    "        #             self.their_moves = []\n",
    "        #             self.hidden_state = self.init_hidden_state()\n",
    "        #             self.rounds = 1\n",
    "        #             self.generate_starting_sequence()\n",
    "        # else:\n",
    "        #     morse = False\n",
    "    \n",
    "    def new_round(self):\n",
    "        self.my_moves = []\n",
    "        self.their_moves = []\n",
    "        self.hidden_state = self.init_hidden_state()\n",
    "        self.rounds = 1\n",
    "        self.print_rounds += 1\n",
    "        self.generate_starting_sequence()\n",
    "            \n",
    "\n",
    "    def get_next_action(self, last_opponent_action: Action) -> Action:\n",
    "        super().get_next_action(last_opponent_action)\n",
    "\n",
    "        if last_opponent_action != None:\n",
    "            self.their_moves.append(last_opponent_action)\n",
    "        else:\n",
    "            if self.rounds != 1 and (1 == 1): # self.print_rounds % 10 == 0:\n",
    "                self.print_stuff()\n",
    "            \n",
    "            # a new round has started. Thus we need to reset everything and start the training again. \n",
    "            self.new_round()\n",
    "            # print(\"\\n hello round :) \\n\")\n",
    "            \n",
    "\n",
    "        if self.rounds > 2*len(self.starting_sequence):\n",
    "            # \"Morser part\": check if I am playing against myself\n",
    "            morse = self.check_morse()\n",
    "            if morse == True:\n",
    "                morse = self.check_fairplay()\n",
    "                if morse == True:\n",
    "                    self.my_moves.append(Action.COOPERATE)\n",
    "                    self.rounds += 1\n",
    "                    self.current_losses = []\n",
    "                    return Action.COOPERATE\n",
    "                else:\n",
    "                    # continue playing, but reset hidden state\n",
    "                    self.hidden_state = self.init_hidden_state()\n",
    "            pass\n",
    "\n",
    "        elif self.rounds > len(self.starting_sequence) and self.rounds <= 2*len(self.starting_sequence):\n",
    "            # \"Morser part\": check if I am playing against myself\n",
    "            morse = self.check_morse()\n",
    "            # sanity check in case the opponent is still someone else (or in case a new round has begun without notice)\n",
    "            if morse == True:\n",
    "                self.my_moves.append(Action.COOPERATE)\n",
    "                self.rounds += 1\n",
    "                self.current_losses = []\n",
    "                return Action.COOPERATE\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            # else just follow the standard sequence since we are not past round 10\n",
    "            # print(self.rounds)\n",
    "            sequence_move = self.starting_sequence[self.rounds-1]\n",
    "            self.my_moves.append(sequence_move)\n",
    "            # print(\"\\n \"+ str(self.my_moves) + \"\\n\")\n",
    "            # print(\"\\n \"+ str(self.their_moves) + \"\\n\")\n",
    "            # self.print_checker += 1\n",
    "            self.rounds += 1\n",
    "            return sequence_move\n",
    "\n",
    "        # analogous to Dropout: re-initialize hidden state every 100 iterations \n",
    "        # if self.rounds % 100 == 0:\n",
    "        # self.hidden_state = self.init_hidden_state()\n",
    "        \n",
    "        # document losses\n",
    "        self.current_losses = []\n",
    "        \n",
    "        # perform forward pass (training the RNN)\n",
    "        for ii in range(-len(self.starting_sequence),-self.LU_Depth-self.Pred_Depth+2,1):\n",
    "            \n",
    "            input1 = self.input_to_onehot(self.my_moves[ii])\n",
    "            input2 = self.input_to_onehot(self.their_moves[ii])\n",
    "            combined_input = torch.cat((input1, input2), dim=1)\n",
    "            if self.LU_Depth > 1:\n",
    "                for LU in range(1,self.LU_Depth):\n",
    "                    input1 = self.input_to_onehot(self.my_moves[ii+LU])\n",
    "                    input2 = self.input_to_onehot(self.their_moves[ii+LU])\n",
    "                    combined_input = torch.cat((combined_input, input1, input2), dim=1)\n",
    "            \n",
    "            # # adjust shape from (input_size) to (batch_size, input_size)\n",
    "            # input1 = input1.unsqueeze(0)\n",
    "            # input2 = input2.unsqueeze(0)\n",
    "                    \n",
    "            # print(input1)\n",
    "            # print(input1.shape)\n",
    "            # input1 = self.input_to_onehot(self.my_moves[ii])\n",
    "            # input2 = self.input_to_onehot(self.their_moves[ii])\n",
    "            # combined_input = torch.cat((input1, input2), dim=2) # dim=1)\n",
    "\n",
    "            # Concatenate input1_expanded and input2_expanded along the third dimension\n",
    "            # combined_input = torch.cat((input1, input2), dim=1)  # shape: (batch_size, input_size)\n",
    "            # print(combined_input)\n",
    "            # print(combined_input.shape)\n",
    "\n",
    "            # self.hidden_state: shape: (batch_size, hidden_size)\n",
    "            # print(self.hidden_state)\n",
    "            # print(self.hidden_state.shape)\n",
    "\n",
    "\n",
    "            # self.forward is called here automatically\n",
    "            # output, hidden = self.forward(input, self.hidden_state)\n",
    "\n",
    "            # Concatenate combined_input and hidden_state_expanded along the second dimension\n",
    "            combined = torch.cat((combined_input, self.hidden_state), dim=1)  # shape: (batch_size, input_size + hidden_size, 1)\n",
    "            self.hidden = self.rnn[0](combined)\n",
    "            output = self.rnn[2](self.hidden_state)\n",
    "\n",
    "            # if Prediction-Depth is >1, multiple future moves are proposed by the RNN and stored in the list my_next_moves \n",
    "            single_output = output.split(int(output.size(dim=1)/self.Pred_Depth), dim=1)\n",
    "\n",
    "            my_next_moves = []\n",
    "            for Pred in range(0,self.Pred_Depth):\n",
    "                # Choose the move with the highest probability\n",
    "                _, predicted_index = torch.max(single_output[Pred], 1)\n",
    "                next_move_index = predicted_index.item()\n",
    "\n",
    "                # Convert the predicted move index to 'COOPERATE' or 'DEFECT'\n",
    "                # Even indices always correspond to COOPERATE, odd indices to DEFECT\n",
    "                if next_move_index % 2 == 0:\n",
    "                    my_next_moves.append(Action.COOPERATE)\n",
    "                else:\n",
    "                    my_next_moves.append(Action.DEFECT)\n",
    "            \n",
    "                # perform backward pass (backpropagation to adjust weights)\n",
    "                # but not on the last move (since the response is unknown yet!)\n",
    "                if ii+self.LU_Depth+self.Pred_Depth-1 != 0:\n",
    "                    \n",
    "                    # start with the first value that corresponds to a predicted step\n",
    "                    first_Pred = ii+self.LU_Depth-1\n",
    "                    # print(type(output))\n",
    "                    # print(type(my_next_moves))\n",
    "                    # print(type(self.their_moves[(first_Pred):(first_Pred-self.Pred_Depth)]))\n",
    "                    self.backward(single_output[Pred], my_next_moves[Pred], self.their_moves[first_Pred+Pred])\n",
    "        \n",
    "        # # if self.rounds % 2000 == 0:\n",
    "        # #     print(\"\\n total loss: \" + str(total_loss) + \"\\n\")\n",
    "\n",
    "        # input1 = self.input_to_onehot(self.my_moves[-1])\n",
    "        # input2 = self.input_to_onehot(self.their_moves[-1])\n",
    "        # input = torch.cat((input1, input2), dim=1)\n",
    "        \n",
    "        # # self.forward is called here automatically\n",
    "        # # output, hidden = self.forward(input, self.hidden_state)\n",
    "        # combined = torch.cat((input, self.hidden_state), 1)\n",
    "        # self.hidden = self.rnn[0](combined)\n",
    "        # output = self.rnn[2](self.hidden_state)\n",
    "\n",
    "        # # Choose the move with the highest probability\n",
    "        # _, predicted_index = torch.max(output, 1)\n",
    "        # my_next_move_index = predicted_index.item()\n",
    "\n",
    "        # # Convert the predicted move index to 'COOPERATE' or 'DEFECT'\n",
    "        # if my_next_move_index == 0:\n",
    "        #     my_next_move = Action.COOPERATE\n",
    "        # elif my_next_move_index == 1:\n",
    "        #     my_next_move = Action.DEFECT\n",
    "\n",
    "        # # perform backward pass (backpropagation to adjust weights)\n",
    "        # # self.backward(output, my_next_move)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # for ii in range(10,0,-1):\n",
    "        #     onehot_moves.append(self.input_to_onehot(self.their_moves[-ii]))\n",
    "        # input = self.input_to_onehot(onehot_moves)\n",
    "        # # self.forward is called here automatically\n",
    "        # # output, hidden = self.forward(input, self.hidden_state)\n",
    "        # combined = torch.cat((input, self.hidden_state), 1)\n",
    "        # self.hidden = self.rnn[0](combined)\n",
    "        # output = self.rnn[2](self.hidden_state)\n",
    "\n",
    "        # # Choose the move with the highest probability\n",
    "        # _, predicted_index = torch.max(output, 1)\n",
    "        # my_next_move_index = predicted_index.item()\n",
    "\n",
    "        # # Convert the predicted move index to 'COOPERATE' or 'DEFECT'\n",
    "        # if my_next_move_index == 0:\n",
    "        #     my_next_move = Action.COOPERATE\n",
    "        # else:\n",
    "        #     my_next_move = Action.DEFECT\n",
    "\n",
    "        # # perform backward pass (backpropagation to adjust weights)\n",
    "        # self.backward(output, my_next_move)\n",
    "        \n",
    "        # document the next move for the reward calculations in the next round\n",
    "        self.my_moves.append(my_next_moves[-1])\n",
    "        self.rounds += 1\n",
    "\n",
    "        return my_next_moves[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy cell\n",
    "import torch\n",
    "\n",
    "some_Pred_Depth = 3\n",
    "\n",
    "some_output = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "print(some_output[(4+1):(4+some_Pred_Depth+1)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend ist das Tournament implementiert. Die Aufgabe der einzelnen Funktionen wird hier zum Verständnis kurz erklärt, diese sind für euch aber wahrscheinlich weniger relevant:\n",
    "\n",
    "- `sample_reward`: gibt ein sample aus einer Gauss-Verteilung zurück, die Varianz 1 hat und über dem Übergabewert zentriert ist, wird genutzt um randomisierte Rewards zu erzeugen\n",
    "- `sample_new_rewards_means`: mit einer Wahrscheinlichkeit von 1% (oder einem anderen übergebnen Wert) werden neue Reward-Mittelwerte gesamplet, die neuen Mittelwerte werden ebenfalls aus einer Normalverteilung gesamplet, die über vorgegeben Werten zentriert ist\n",
    "- `get_reward`: stellt sicher, dass die übergebenen Aktionen der Agenten valide sind und gibt dann basierend auf den Aktionen passende Reward samples zurück\n",
    "- `get_uncertain_observation`: mit einer Wahrscheinlichkeit von 10% werden die Aktionen getauscht, um Unsicherheit über die gewählte Aktion des Gegner zu erzeugen\n",
    "- `play_rounds`: eine zufällige Anzahl an Zeitschritten/Runden wird gesamplet (bei fester Rundenzahl ist die optimale Strategie immer \"always defect\", siehe Video oben), dann wählen beide übergebenen Agenten für jeden Zeitschritt je eine Aktion, für jedes Aktionspaar erhält der Agent einen Reward, der kumuliert und zum Schluss (zur Bildung eines Mittelwerts) durch die Anzahl der Zeitschritte geteilt wird\n",
    "- `tournament`: für eine feste Anzahl an Wiederholungen (100) spielt jeder Agent einmal gegen jeden Agenten inklusive sich selbst und die Baselines, die normalisierten, kumulierten Rewards werden aufsummiert, in einem dict gespeichert und final, zur Bildung eines Mittelwerts, durch die Anzahl der Wiederholungen geteilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# rewards are sampled from a gaussian distribution centered around the provided mean value \n",
    "def sample_reward(mean: float) -> float:\n",
    "    return random.gauss(mu=mean, sigma=1)\n",
    "\n",
    "# with a probability of 1% we sample new reward means\n",
    "def sample_new_reward_means(curr: list, means: list=[-2, -8, 0, -5], prob: float=0.01) -> float:\n",
    "    if random.random() < prob or curr is None:\n",
    "        curr = []\n",
    "        for m in means:\n",
    "            curr.append(random.gauss(mu=m, sigma=1))\n",
    "\n",
    "    return curr\n",
    "\n",
    "def get_reward(my_action: Action, opponent_action: Action, reward_means: list) -> float:\n",
    "    assert my_action in Action\n",
    "    assert opponent_action in Action\n",
    "    reward_means = sample_new_reward_means(curr=reward_means)\n",
    "\n",
    "    # assume negative numbers are years spent in prison e.g. -1 is one year in prison and 0 is no prison time\n",
    "    # consequently highest value is best\n",
    "    if my_action == Action.COOPERATE and opponent_action == Action.COOPERATE:\n",
    "        return sample_reward(reward_means[0]), reward_means\n",
    "    if my_action == Action.COOPERATE and opponent_action == Action.DEFECT:\n",
    "        return sample_reward(reward_means[1]), reward_means\n",
    "    if my_action == Action.DEFECT and opponent_action == Action.COOPERATE:\n",
    "        return sample_reward(reward_means[2]), reward_means\n",
    "    if my_action == Action.DEFECT and opponent_action == Action.DEFECT:\n",
    "        return sample_reward(reward_means[3]), reward_means\n",
    "\n",
    "# with a probability of 10% the agent receives a wrong observation\n",
    "def get_uncertain_observation(action: Action) -> Action:\n",
    "    observation = action\n",
    "    if random.random() < 0.1:\n",
    "        if observation == Action.DEFECT:\n",
    "            observation = Action.COOPERATE\n",
    "        if observation == Action.COOPERATE:\n",
    "            observation = Action.DEFECT\n",
    "    return observation\n",
    "\n",
    "def play_rounds(agent: Agent, opponent: Agent) -> float:\n",
    "    num_rounds = random.randint(200, 400)\n",
    "    action_agent = None\n",
    "    action_opponent = None\n",
    "    reward = 0\n",
    "    reward_means = sample_new_reward_means(curr=None)\n",
    "\n",
    "    for _ in range(num_rounds):\n",
    "        action_opponent = get_uncertain_observation(action_opponent)\n",
    "        action_agent = get_uncertain_observation(action_agent)\n",
    "        action_agent = agent.get_next_action(action_opponent)\n",
    "        action_opponent = opponent.get_next_action(action_agent)\n",
    "\n",
    "        c_reward, reward_means = get_reward(action_agent, action_opponent, reward_means)\n",
    "        reward += c_reward\n",
    "\n",
    "    return reward/num_rounds\n",
    "\n",
    "def tournament(all_agents: list, repeats: int=50) -> dict:\n",
    "    results = {x.__class__.__name__: 0 for x in all_agents}\n",
    "\n",
    "    # we repeat the experiment multiple times to eliminate \"luck\" as far as possible\n",
    "    for _ in tqdm(range(repeats), desc='repeats'):\n",
    "        # each agent plays each opponent once per repeat\n",
    "        for agent in all_agents:\n",
    "            # print(\"\\n\" + str(agent))\n",
    "            for opponent in all_agents:\n",
    "                # print(str(opponent) + \"\\n\")\n",
    "                r = play_rounds(agent, opponent)\n",
    "                results[agent.__class__.__name__] += r\n",
    "\n",
    "    # normalization just to get smaller numbers\n",
    "    for key in results.keys():\n",
    "        results[key] /= repeats\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem Nachfolgendem Code wird das Tournament ausgeführt und ausgewertet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agents = [Real_RNN_Agent_31(), DefectAgent(), CooperateAgent(), SwitchingAgent(), RandomAgent()]  # all of your agents will be added to this list\n",
    "results = tournament(all_agents)\n",
    "results_sorted = dict(sorted(results.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print('RESULTS:') # highest value is best (all will be <= 0)\n",
    "for i, agent_key in enumerate(results_sorted.keys()):\n",
    "    print(f'\\t {i+1}. {agent_key}: {round(results_sorted[agent_key], 4)}')\n",
    "print(f'Winner is {max(results, key=results.get)} 🥳🥳🥳🥳')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonforeverything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
